{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library\n",
    "- http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/\n",
    "- http://learningtensorflow.com/index.html\n",
    "- http://suriyadeepan.github.io/2016-12-31-practical-seq2seq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Tensorflow basic\n",
    "\n",
    "- **tf.constant(), tf.Variable()**: 그래프의 객체\n",
    "- **tf.Session()/ tf.InteractiveSession()**: 그래프를 시작\n",
    "- **sess.run(c)**: 텐서 'c'를 계산 -> sess.close() 해야함\n",
    "- **with 절**: tf.Session()은 with 절과 사용, with 절에서 sess.close() 생략가능\n",
    "- **c.eval()**: sess.run(c)와 같음\n",
    "\n",
    "### - **tf.variable_scope()**\n",
    "- 모델에서 필요한 변수들을 관리하는 클래스(변수 범위 만들기)\n",
    "- tf.get_variable(): 직접호출 없이 변수를 가져오거나 생성(initializer 사용)\n",
    "- reuse = False: '현재 variable scope 이름 + 제공된 name'이 없으면 생성\n",
    "- reuse = True: '현재 variable scope 이름 + 제공된 name'이 있으면 반환\n",
    "- reuse = AUTO_REUSE: 둘다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Long Sequence RNN(automatic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h', 'y', 'm', 'l', 'e', 'i', ' ', 'a', 'o', 'n', 'H', 's', 'j']\n",
      "{'h': 0, 'y': 1, 'm': 2, 'l': 3, 'e': 4, 'i': 5, ' ': 6, 'a': 7, 'o': 8, 'n': 9, 'H': 10, 's': 11, 'j': 12}\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sample = \"Hello my name is hyejin\"\n",
    "idx2char = list(set(sample))  # index -> char\n",
    "# 중복 제거\n",
    "char2idx = {c: i for i, c in enumerate(idx2char)}  # char -> idex\n",
    "# character별 숫자 딕셔너리\n",
    "print(idx2char)\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 13 13\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "# hyper parameters\n",
    "dic_size = len(char2idx)  # RNN input size (one hot size)\n",
    "rnn_hidden_size = len(char2idx)  # RNN output size\n",
    "num_classes = len(char2idx)  # final output size (RNN or softmax, etc.)\n",
    "\n",
    "batch_size = 1  # one sample data, one batch\n",
    "sequence_length = len(sample) - 1  # number of lstm rollings (unit #)\n",
    "learning_rate = 0.1\n",
    "print(dic_size, rnn_hidden_size, num_classes)\n",
    "print(sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 22), dtype=int32)\n",
      "Tensor(\"Placeholder_1:0\", shape=(?, 22), dtype=int32)\n",
      "Tensor(\"one_hot:0\", shape=(?, 22, 13), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sample_idx = [char2idx[c] for c in sample]  # char to index\n",
    "x_data = [sample_idx[:-1]]  # X data sample (0 ~ n-1) hello: hell\n",
    "y_data = [sample_idx[1:]]   # Y label sample (1 ~ n) hello: ello\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length])  # X data\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])  # Y label\n",
    "                            #batch_size = 1\n",
    "\n",
    "# flatten the data (ignore batches for now). No effect if the batch size is 1\n",
    "x_one_hot = tf.one_hot(X, num_classes)  # one hot: 1 -> 0 1 0 0 0 0 0 0 0 0\n",
    "print(X)\n",
    "print(Y)\n",
    "print(x_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=rnn_hidden_size, state_is_tuple=True)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 22, 13)\n",
      "(1, 22, 13)\n"
     ]
    }
   ],
   "source": [
    "# reshape out for sequence_loss\n",
    "print(outputs.shape)\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
    "print(outputs.shape)\n",
    "\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=Y, weights=weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "prediction = tf.argmax(outputs, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 2.5710404 Prediction: en mmmmmmmmmmmm miiiin\n",
      "1 loss: 2.4674807 Prediction: e             n     nn\n",
      "2 loss: 2.340106 Prediction: e          e in iyijin\n",
      "3 loss: 2.270538 Prediction: e    m  m             \n",
      "4 loss: 2.2247298 Prediction: e               iyeiin\n",
      "5 loss: 2.158218 Prediction: e                   in\n",
      "6 loss: 2.1276743 Prediction: e                     \n",
      "7 loss: 2.0745318 Prediction: eooo m                \n",
      "8 loss: 2.021412 Prediction: eooo m                \n",
      "9 loss: 1.9701705 Prediction: eooo m               n\n",
      "10 loss: 1.9194913 Prediction: eooo m           y  in\n",
      "11 loss: 1.8725915 Prediction: elll m        n hy  in\n",
      "12 loss: 1.823976 Prediction: elll m       nn hy jin\n",
      "13 loss: 1.7782437 Prediction: elll m  n me nn hyejin\n",
      "14 loss: 1.7366173 Prediction: elll my n me in hyejin\n",
      "15 loss: 1.7046306 Prediction: elll my nmme in hyejin\n",
      "16 loss: 1.6776389 Prediction: elll my nmme in hyejin\n",
      "17 loss: 1.6515915 Prediction: elll my nmme in hyejin\n",
      "18 loss: 1.6263705 Prediction: elll my nmme in hyejin\n",
      "19 loss: 1.6036464 Prediction: elll my nmme nn hyejin\n",
      "20 loss: 1.5838844 Prediction: elll my nmme nn hyejin\n",
      "21 loss: 1.5667686 Prediction: elll my nmme nn hyejin\n",
      "22 loss: 1.552613 Prediction: elll my n me nn hyejin\n",
      "23 loss: 1.5402869 Prediction: elll my n me nn hyejin\n",
      "24 loss: 1.5282515 Prediction: elll my n me nn hyejin\n",
      "25 loss: 1.5170155 Prediction: elll my nmme nn hyejin\n",
      "26 loss: 1.5064743 Prediction: elll my nmme in hyejin\n",
      "27 loss: 1.4973165 Prediction: elll my nmme in hyejin\n",
      "28 loss: 1.4890594 Prediction: elll my nmme in hyejin\n",
      "29 loss: 1.4817075 Prediction: ello my nmme in hyejin\n",
      "30 loss: 1.4747998 Prediction: ello my nmme in hyejin\n",
      "31 loss: 1.4686047 Prediction: ello my nmme in hyejin\n",
      "32 loss: 1.4629319 Prediction: ello my nmme in hyejin\n",
      "33 loss: 1.4577659 Prediction: ello my nmme in hyejin\n",
      "34 loss: 1.4527115 Prediction: ello my nmme is hyejin\n",
      "35 loss: 1.4475814 Prediction: ello my nmme is hyejin\n",
      "36 loss: 1.4421006 Prediction: ello my nmme is hyejin\n",
      "37 loss: 1.4371845 Prediction: ello my nmme is hyejin\n",
      "38 loss: 1.4325674 Prediction: ello my nmme is hyejin\n",
      "39 loss: 1.4279336 Prediction: ello my nmme is hyejin\n",
      "40 loss: 1.4233546 Prediction: ello my nmme is hyejin\n",
      "41 loss: 1.4192122 Prediction: ello my nmme is hyejin\n",
      "42 loss: 1.4154471 Prediction: ello my nmme is hyejin\n",
      "43 loss: 1.4119767 Prediction: ello my nmme is hyejin\n",
      "44 loss: 1.4085649 Prediction: ello my nmme is hyejin\n",
      "45 loss: 1.40512 Prediction: ello my nmme is hyejin\n",
      "46 loss: 1.401897 Prediction: ello my nmme is hyejin\n",
      "47 loss: 1.3989586 Prediction: ello my nmme is hyejin\n",
      "48 loss: 1.3962926 Prediction: ello my nmme is hyejin\n",
      "49 loss: 1.3936408 Prediction: ello my nmme is hyejin\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X: x_data, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict={X: x_data})\n",
    "\n",
    "        # print char using dic\n",
    "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "\n",
    "        print(i, \"loss:\", l, \"Prediction:\", ''.join(result_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
